# 虚拟化演变
## 虚拟化层翻译
### x86平台的指令集权限划分
x86平台指令集划分为4个特权模式：Ring 0、Ring1、Ring2、Ring3。操作系统一般使用Ring 0级别，应用程序使用Ring 3级别，驱动程序使用Ring 1和Ring 2级别。x86平台在虚拟化方面的一个难点就是**如何将虚拟机越级的指令使用进行隔离。**

通过虚拟化引擎，捕获虚拟机的指令，并进行处理，这也是为什么在虚拟机上虽然使用的是物理机一样的指令，但是虚拟机不能对硬件进行操作的原因，比如重启虚拟机不会引起宿主机的重启。这种解决方案也叫软件全虚拟化方案。（虚拟化引擎VMM）
## 改造虚拟机操作系统的方式
对虚拟机的操作系统内核进行改造，使虚拟机自己对特殊的指令进行更改，然后和虚拟化层一起配合工作，这也是Xen早期一直使用一个要使用一个特殊内核的原因，并且不支持Windows系统虚拟化。改造的虚拟机虽然使用上有限制，配置比较麻烦，但是这种方法效率非常高，这种方式也被称为半虚拟化方案。
## 硬件虚拟化
2005年，Intel推出了硬件的方案，对CPU指令进行改造，即VT-X，VT-x增加了两种操作模式：VMX root operation和VMX non-root operation。VMM运行在VMX root operation模式，虚拟机运行在VMX non-root operation模式。这两种模式都支持Ring 0 ~ Ring3 这4个特权级。
这种方案是基于硬件的，所以效率非常高，这种方案也称为硬件支持的全虚拟化方案。现在的一个发展是不仅CPU指令有硬件解决方案，I/O通信也有硬件解决方案，称为VT-d;网络通信也有硬件解决方案，称为VT-c。
## 容器虚拟化
容器虚拟化的原理是基于CGroups、Namespace等技术将进程隔离，每个进程就像一台单独的虚拟机一样，有自己被隔离出来的资源，也有自己的根目录、独立的进程编号、被隔离的内存空间。基于窗口的虚拟化可以实现在单一内核上运行多个实例，因此是一个更高效率的虚拟化方式。但是Docker在生产环境的使用还需要一个过程，主要是磁盘、网络性能上还受到很多限制。

# KVM的架构
KVM的架构非常简单，KVM就是内核的一个模块，用户空间通过QEMU模拟硬件提供给虚拟机使用，一台虚拟机就是一个普通的Linux进程，通过对这个进程的管理，就可以完成对虚拟机的管理。实际上德国有家公司开发了一个管理平台ProxmoxVE,就是通过对KVM进程的管理来实现对虚拟机管理的。
因为对进程的管理非常麻烦，RedHat发布了一个项目Libvirt。Libvirt有API，也有一套命令行工具，可以完成对虚拟机的管理，大多数的管理平台都是通过Libvirt来完成对KVM虚拟机的管理的，如OpenStack等。
## QEMU与KVM
QEMU是一个项目，实际就是一台硬件虚拟器，可以模拟许多硬件。
QEMU可以在其他平台上运行Linux的程序，可以存储及还原虚拟机运行状态。QEMU的好处是因为是纯软件模拟，所以可以在支持的平台模拟支持的设备。QEMU的缺点是因为是纯软件模拟，所以非常慢。KVM只是一个内核的模块，没有用户空间的管理工具。KVM的虚拟机可以借助QEMU的管理工具来管理。QEMU也可以借助KVM来加速，提升虚拟虚拟机的性能。现在QEMU的版本
> KVM的最后一个自己的版本是KVM 83,随后和内核版本一起发布，和内核版本号保持一致，所以要使用KVM的最新版本就要使用最新的内核。

## Libvirt与KVM
Libvirt是一套开源的虚拟化的管理工具，主要由3部分组成：
* 一套API的Lib库，支持主流的编程语言，包括C、Python、R等。
* Libvirtd服务
* 命令行工具virsh

Libvirt的设计目标是通过相同的方式管理不同的虚拟化引擎，但是现在一般用于KVM的管理，因为其他的虚拟化都有自己的管理工具

Libvirt可以实现对虚拟机的管理，比如虚拟机的创建、启动、关闭、暂停、恢复、迁移、销毁以及虚拟机网卡、硬盘、CPU、内存等多种设备的热添加。

Libvirt将虚拟机的管理分为以下几个方面：
1. 存储池资源管理，支持本地文件系统目录、裸设备、LVM、NFS、ISCSI等方式。在虚拟机磁盘模式上支持qcow2、vmdx、raw等格式。
2. 网络资源管理，支持Linux桥、VLAN、多网卡绑定管理，比较新的版本还支持Open vSwitch。Libvirt还支持nat和路由方式的网络，Libvirt可以通过防火墙让虚拟机通过宿主机建立网络通道和外部的网络进行通信。

# 搭建第一台KVM虚拟机
首先需要新建一台虚拟机，然后通过诸如物理机安装的方式为虚拟机安装操作系统。但是一般不会使用pxe、cobbler等网络引导方式安装，因为这样安装虚拟机系统太慢了。虚拟化相比于物理机，其中一个优势就是创建快速。所以一般都会使用ISO镜像文件安装第一台虚拟机，然后将这台虚拟机做成虚拟机的，之后的虚拟机都是由这个模板生成的。
## 开启CPU虚拟机
bios里面设置`CPU Virtualization Technology`开启vt-x,可以通过下面命令查看CPU是否支持虚拟机

	egrep '(vmx|svm) /proc/CPUinfo'

## 检查并安装相关软件包

	rpm -qa | grep -E 'qemu-img|libvirt-[0-9]|virt-install'
> qemu-img.x86_64 : QEMU command line tool for manipulating disk images
> 
> virt-install.noarch : Utilities for installing virtual machines
> 
> libvirt.x86_64 : Library providing a simple virtualization API
## 检查KVM模块是否载入
	lsmod | grep kvm

## 两个KVM虚拟化中常用的管理并创建虚拟机
### Virt-Manager是一个图形化的虚拟机管理工具，它提供了一个简易的虚拟机操作界面。要使用它，需要先安装图形化界面。

### virt-install命令使用介绍
virt-install是一个在命令行创建KVM虚拟机的工具，使用virt-install配合一些配置，最终可以生成一个完整的.xml虚拟机配置文件

	virt-install --name=testvm --ram=2048 --vCPUs=4 --os-type=Windows --hvm --cdrom=/root/W2003cnet.iso --file=/root/SDG100.img --file-size=10 --bridge=br0 --vnc --vncport=5920

参数说明如下：
<pre>
--name: 设置虚拟机名称

--ram: 配置虚拟机内存，单位是MB

--vCPUs: 配置CPU个数

--hvm: 配置使用全虚拟化

--os-type: 指定操作系统类型，如Linux,Windows

--cdrom: 使用cdrom安装系统，指定ISO位置 

--file: 设置虚拟机硬盘文件路径 

--bridge: 配置桥接的网卡

--vnc: 打开VNC支持

--vncport: 配置VNC端口
</pre>
执行上述命令之后，virt-install会创建一台名为testvm的虚拟机，并使用W2003cnent.ios镜像文件安装系统。此时使用VNC Viewer,在VNC Server中输入宿主机ip:vncport，便可登录虚拟机的控制台，此时虚拟机开始从ISO引导，安装虚拟机系统的步骤和安装普通服务器系统是一样的。

#### Windows虚拟机安装注意事项
第一次安装Windows虚拟机的时候，经常会碰到以下几个问题:

* qcow2格式的磁盘如何操作
virt-Manager默认创建的磁盘格式是RAW格式，如果需要使用qcow2格式的磁盘，必须用 `qemu-img create` 手工先创建一个qcow2格式的磁盘镜像
	qemu-img create Windows-test.qcow2 -f qcow2 50G

然后在Virt-Manager上指定qcow2格式。

> 在使用virt-install命令，磁盘镜像格式为qcow2时，在virt-install命令中要特别指明磁盘格式，否则会出现镜像复制之后虚拟机系统不能启动的现象。

# 调优
KVM虚拟机CPU的软件调优首先需要对NUMA技术有一定的了解，调优的主要手段就是虚拟机对物理机CPU逻辑核的手工绑定。
CPU的Nested我使用也是非常有意思的一个特性，KVM虚拟机的嵌套在理论上可以无限层地嵌套下去，只要物理机的性能足够。
内在方面的调优手段主要是KSM，即相同内存页合并、内在气球技术及大页内存的使用。

## NUMA技术与应用 
NUMA是一种解决多CPU共同工作的技术方案，首先回顾一下多CPU共同工作技术的架构历史。多CPU共同工作主要有3种架构，分别是SMP、MPP、NUMA架构。SMP、MPP、NUMA都是为了解决多CPU共同工作的问题。
### SMP技术
早期的时候，每台服务器都是单CPU，随着技术的发展，出现了多CPU共同工作的需求，最早的多CPU技术是SMP。
SMP即多个CPU通过一个总线访问存储器，因此SMP系统有时也被称为一致内在访问（UMA）结构体系。一致性意指无论在什么时候，处理器只能为内在的每个数据保持或共享唯一一个数值。
SMP的缺点是扩展性有限，因为在存储器接口达到饱和的时候，增加处理器并不能获得更高的性能，因此SMP方式支持的CPU个数有限。
### MPP模式
MPP模式则是一种分布式储存器模式，能够将更多的处理器纳入一个系统的存储器。一个分布式存储器模式具有多个节点，每个节点都有自己的存储器，可以配置为SMP模式，也可以配置为非SMP模式。单个节点相互连接起来就形成了一个总系统。MPP可以挖理解成一个SMP的横向扩展集群。MPP一般要依靠软件实现。
### NUMA技术
NUMA模式则是每个处理器有自己的存储器，每个处理器也可以访问别的处理器的存储器。
<pre>
yum install numactl -y
numactl --hardware
available: 1 nodes (0)
node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
node 0 size: 65490 MB
node 0 free: 47946 MB
node distances:
node   0 
  0:  10 
</pre>
> 有一颗CPU,这颗CPU有16个核，每个核有64GB内存可以用

使用numastat命令可以查看每个节点的内存统计
<pre>
[root@localhost ~]# numastat 
                           node0
numa_hit                34201738	# 使用本节点内存次数
numa_miss                      0	# 计划使用本节点内存而被高度到其他节点次数
numa_foreign                   0	# 计划使用其他节点内存而使用本地内存次数
interleave_hit             44068	# 交叉分配使用的内存中使用本地节点内存次数
local_node              34201738	# 在本节点运行的程序使用本节点的内存次数
other_node                     0	# 在其他节点运行的程序使用本节点内存次数
</pre>

numastat命令使用-c参数可以查看相关进程的NUMA内存使用情况
<pre>
[root@localhost ~]# numastat -c qemu-kvm

Per-node process memory usage (in MBs) for PID 22323 (qemu-kvm)
         Node 0 Total
         ------ -----
Huge          0     0
Heap         76    76
Stack         2     2
Private     956   956
-------  ------ -----
Total      1033  1033
</pre>
centos7系统默认是关闭自动NUMA平衡策略。如果要开启Linux系统的自动平衡，可以使用如下命令：
<pre>echo 1 > /proc/sys/kernel/numa_balancing</pre>
## CPU Nested技术与配置方法
Nested技术，简单地说，就是在虚拟机上运行虚拟机，即KVM ON KVM，KVM虚拟机嵌套和VMWARE原理不同，VMWARE第一层是用的硬件虚拟化技术，第二层就是完全软件模拟出来的，所以VMWARE只能做两层嵌套。KVM是将物理CPU的特性全部传给虚拟机，所以理论上嵌套N多层，但是事实上测试的时候，跑了两层就很慢了。Centos7官方宣称不正式支持Nested技术。
* 主机解析

`/etc/hosts`
<pre>
192.168.8.145	node
192.168.8.146   node1
192.168.8.194   node2
192.168.8.183   node3
</pre>

## keystone

<pre>
yum install openstack-keystone openstack-utils httpd mod_wsgi python-keystone python-openstackclient memcached python-memcached -y
mysql -uroot -p -h192.168.8.145
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'controller' IDENTIFIED BY 'keystone';
openstack-config --set /etc/keystone/keystone.conf database connection mysql+pymysql://keystone:keystone@node/keystone
openstack-config --set /etc/keystone/keystone.conf token provider fernet
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_token ADMIN_TOKEN
openstack-config --set /etc/keystone/keystone.conf DEFAULT debug true
openstack-config --set /etc/keystone/keystone.conf DEFAULT admin_bind_host $(hostname -i|awk '{print $2}')
openstack-config --set /etc/keystone/keystone.conf DEFAULT public_bind_host $(hostname -i|awk '{print $2}')
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
chown -R keystone:keystone /etc/keystone /var/lib/keystone /var/log/keystone
systemctl enable openstack-keystone
systemctl start openstack-keystone
export OS_TOKEN=ADMIN_TOKEN
export OS_URL=http://node:35357/v3
export OS_IDENTITY_API_VERSION=3
openstack service create --name keystone --description "OpenStack Identity" identity
openstack endpoint create --region wuhan identity public http://node:5000/v3
openstack endpoint create --region wuhan identity internal http://node:5000/v3
openstack endpoint create --region wuhan identity admin http://node:35357/v3
openstack role create admin
openstack role create service
openstack role create domain_admin
openstack role create project_admin
openstack role create guest
openstack role create member
export DEFAULT_DOMAIN_ID=`openstack domain create default | grep -w id | awk '{print $4}'`
openstack-config --set /etc/keystone/keystone.conf identity default_domain_id $DEFAULT_DOMAIN_ID
unset DEFAULT_DOMAIN_ID
systemctl restart openstack-keystone
openstack project create --domain default --description "Admin Project" admin
openstack project create --domain default --description "Service Project" service
  
openstack user create --domain default --project admin --project-domain default --password admin admin
openstack role add --domain default --user admin --project-domain default --user-domain default admin --inherited
openstack role add --project admin --user admin --project-domain default --user-domain default admin
unset OS_TOKEN OS_URL
unset OS_URL
unset OS_IDENTITY_API_VERSION
cat >/root/keystone_admin_v3 &lt;&lt; EOF
export OS_PROJECT_DOMAIN_NAME=default
export OS_USER_DOMAIN_NAME=default
export OS_PROJECT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=admin
export OS_AUTH_URL=http://node:35357/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
export OS_ENDPOINT_TYPE=internal
export OS_INTERFACE=internal
export PS1='[\u@\h \W(keystone_admin_v3)]$'
EOF
source /root/keystone_admin_v3
openstack token issue
scp /root/keystone_admin_v3 node2:/root/
scp /root/keystone_admin_v3 node3:/root/
scp /etc/keystone/keystone.conf node2:/etc/keystone/keystone.conf 
scp /etc/keystone/keystone.conf node3:/etc/keystone/keystone.conf
scp -r /etc/keystone/fernet-keys node3:/etc/keystone 
scp -r /etc/keystone/fernet-keys node2:/etc/keystone 
ssh node2 chown -R keystone:keystone /etc/keystone /var/lib/keystone /var/log/keystone
ssh node3 chown -R keystone:keystone /etc/keystone /var/lib/keystone /var/log/keystone
ssh node2 systemctl enable openstack-keystone
ssh node2 systemctl start openstack-keystone
ssh node3 systemctl enable openstack-keystone
ssh node3 systemctl start openstack-keystone
</pre>

> 复制完配置文件后需要修改每个节点的绑定地址

### 修改 `Haproxy` 配置文件
<pre>
############# keystone_admin_cluster ########
listen keystone_admin_cluster
bind 192.168.8.145:35357
balance  source
option  tcpka
option  httpchk
option  tcplog
server node1 192.168.8.146:35357 check inter 2000 rise 2 fall 5
server node2 192.168.8.194:35357 check inter 2000 rise 2 fall 5
server node3 192.168.8.183:35357 check inter 2000 rise 2 fall 5

listen keystone_public_internal_cluster
bind 192.168.8.145:5000
balance  source
option  tcpka
option  httpchk
option  tcplog
server node1 192.168.8.146:5000 check inter 2000 rise 2 fall 5
server node2 192.168.8.194:5000 check inter 2000 rise 2 fall 5
server node3 192.168.8.183:5000 check inter 2000 rise 2 fall 5
</pre>

## Glance
<pre>
yum install openstack-glance python-glance openstack-utils -y
mysql
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'node' IDENTIFIED BY 'glance';
openstack service create --name glance  image
openstack endpoint create --region wuhan image public http://node:9292
openstack endpoint create --region wuhan image internal http://node:9292
openstack endpoint create --region wuhan image admin http://node:9292
openstack user create --domain default --project service --project-domain default --password glance glance
openstack role add --project service --user glance --project-domain default --user-domain default admin
openstack-config --set /etc/glance/glance-api.conf DEFAULT show_image_direct_url True
openstack-config --set /etc/glance/glance-api.conf DEFAULT show_multiple_locations True
openstack-config --set /etc/glance/glance-api.conf DEFAULT workers 2
openstack-config --set /etc/glance/glance-api.conf DEFAULT debug True
openstack-config --set /etc/glance/glance-api.conf database connection mysql+pymysql://glance:glance@node/glance
openstack-config --set /etc/glance/glance-api.conf glance_store stores rbd
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_pool images
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_user glance
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_ceph_conf /etc/ceph/ceph.conf
openstack-config --set /etc/glance/glance-api.conf glance_store rbd_store_chunk_size 8
openstack-config --set /etc/glance/glance-api.conf glance_store default_store rbd
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_uri http://node:5000
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_url http://node:35357
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-api.conf keystone_authtoken service_token_roles_required true
openstack-config --set /etc/glance/glance-api.conf paste_deploy flavor keystone

openstack-config --set /etc/glance/glance-registry.conf DEFAULT workers 2
openstack-config --set /etc/glance/glance-registry.conf database connection mysql+pymysql://glance:glance@node/glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://node:5000
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_url http://node:35357
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken auth_type password
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken project_name service
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken username glance
openstack-config --set /etc/glance/glance-registry.conf keystone_authtoken password glance
openstack-config --set /etc/glance/glance-registry.conf paste_deploy flavor keystone
</pre>
<pre>
glance-manage db sync
chown -R glance.glance /etc/glance /var/lib/glance /var/log/glance
scp /etc/glance/glance-api.conf node2:/etc/glance/glance-api.conf
scp /etc/glance/glance-api.conf node3:/etc/glance/glance-api.conf
ssh node2 chown -R glance.glance /etc/glance /var/lib/glance /var/log/glance
ssh node3 chown -R glance.glance /etc/glance /var/lib/glance /var/log/glance
for id in openstack-glance-{api,registry};do systemctl enable $id;systemctl start $id;done
ssh node2 for id in openstack-glance-{api,registry};do systemctl enable $id;systemctl start $id;done
ssh node3 for id in openstack-glance-{api,registry};do systemctl enable $id;systemctl start $id;done
glance image-list
wget http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
glance image-create --name cirros --disk-format qcow2 --container-format bare --visibility public --file cirros-0.3.5-x86_64-disk.img
</pre>
<pre>
rabbit_hosts = 192.168.8.146:$rabbit_port, 192.168.8.194:$rabbit_port, 192.168.8.183:$rabbit_port
bind_host = 192.168.8.183
</pre>
## cinder
<pre>
yum install openstack-cinder python-cinder openstack-utils -y
mysql
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'node' IDENTIFIED BY 'cinder';
openstack service create --name cinderv2 volumev2
openstack endpoint create --region wuhan   volumev2 admin http://node:8776/v2/%\(tenant_id\)s
openstack endpoint create --region wuhan   volumev2 public http://node:8776/v2/%\(tenant_id\)s
openstack endpoint create --region wuhan   volumev2 internal http://node:8776/v2/%\(tenant_id\)s
openstack-config --set /etc/cinder/cinder.conf DEFAULT transport_url rabbit://guest:guest@node
openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_version 2
openstack-config --set /etc/cinder/cinder.conf DEFAULT glance_api_servers http://node:9292
openstack-config --set /etc/cinder/cinder.conf DEFAULT default_volume_type ceph
openstack-config --set /etc/cinder/cinder.conf DEFAULT my_ip 192.168.122.1 # 设置为storage_net的ip
openstack-config --set /etc/cinder/cinder.conf DEFAULT osapi_volume_listen 192.168.8.146
openstack-config --set /etc/cinder/cinder.conf DEFAULT enabled_backends ceph
openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone
 
openstack-config --set /etc/cinder/cinder.conf database connection mysql+pymysql://cinder:cinder@node/cinder
openstack-config --set /etc/cinder/cinder.conf database max_retries -1
 
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_uri http://node:5000
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_url http://node:35357
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken auth_type password
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken project_name service
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken username cinder
openstack-config --set /etc/cinder/cinder.conf keystone_authtoken password cinder
openstack-config --set /etc/cinder/cinder.conf oslo_concurrency lock_path /var/lib/cinder/tmp

openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_hosts 192.168.8.146:5672,192.168.8.194:5672,192.168.8.183:5672
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit rabbit_ha_queues True
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit heartbeat_timeout_threshold 60
openstack-config --set /etc/cinder/cinder.conf oslo_messaging_rabbit heartbeat_rate 2
openstack-config --set /etc/cinder/cinder.conf ceph volume_backend_name ceph
openstack-config --set /etc/cinder/cinder.conf ceph rbd_secret_uuid 3947ff89-361e-4622-98fd-48b0b2723a6c
openstack-config --set /etc/cinder/cinder.conf ceph rbd_user cinder
openstack-config --set /etc/cinder/cinder.conf ceph rbd_pool volumes
openstack-config --set /etc/cinder/cinder.conf ceph rbd_ceph_conf /etc/ceph/ceph.conf
openstack-config --set /etc/cinder/cinder.conf ceph recalculate_allocated_capacity=True
openstack-config --set /etc/cinder/cinder.conf ceph recalculate_allocated_capacity True
openstack-config --set /etc/cinder/cinder.conf ceph rbd_max_clone_depth 3
openstack-config --set /etc/cinder/cinder.conf ceph volume_backend_name ceph
openstack-config --set /etc/cinder/cinder.conf ceph rados_connect_timeout -1
openstack-config --set /etc/cinder/cinder.conf ceph volume_driver cinder.volume.drivers.rbd.RBDDriver
openstack-config --set /etc/cinder/cinder.conf ceph rbd_flatten_volume_from_snapshot False
openstack-config --set /etc/cinder/cinder.conf ceph rbd_store_chunk_size 4
openstack-config --set /etc/cinder/cinder.conf ceph backend_host volumes
</pre>
<pre>
cinder-manage db sync
chown -R cinder:cinder /etc/cinder/ /var/lib/cinder/ /var/log/cinder/
for id in openstack-cinder-{api,scheduler,volume};do systemctl enable $id;systemctl start $id;done
scp /etc/cinder/cinder.conf node2:/etc/cinder ## 修改存储地址和bind地址
scp /etc/cinder/cinder.conf node3:/etc/cinder
ssh node2 chown -R cinder.cinder /etc/cinder /var/lib/cinder/ /var/log/cinder/
ssh node3 chown -R cinder.cinder /etc/cinder /var/lib/cinder/ /var/log/cinder/
ssh node2 for id in openstack-cinder-{api,scheduler,volume};do systemctl enable $id;systemctl start $id;done
ssh node3 for id in openstack-cinder-{api,scheduler,volume};do systemctl enable $id;systemctl start $id;done
cinder list
cinder create 1
</pre>


## ceph
<pre>
192.168.122.1	storage1
192.168.122.2	storage2
192.168.122.3	storage3
</pre>
<pre>
yum install ceph -y
[root@node1 yum.repos.d(keystone_admin_v3)]$cat /etc/ceph/ceph.conf 
[global]
fsid = 97e61689-e729-45b7-baa8-7e1a36b6b863
public network = 192.168.122.0/24
cluster network = 192.168.122.0/24
auth cluster required = cephx
auth service required = cephx
auth client required = cephx
mon initial members = node1, node2, node3 
mon host = 192.168.122.1, 192.168.122.2, 192.168.122.3 
osd pool default size = 3
osd pool default min size = 1
osd pool default pg num = 333
osd pool default pgp num = 333
osd crush chooseleaf type = 1
ms_type=async
debug_lockdep = 0/0
debug_context = 0/0
debug_crush = 0/0
debug_buffer = 0/0
debug_timer = 0/0
debug_filer = 0/0
debug_objecter = 0/0
debug_rados = 0/0
debug_rbd = 0/0
debug_journaler = 0/0
debug_objectcatcher = 0/0
debug_client = 0/0
debug_osd = 0/0
debug_optracker = 0/0
debug_objclass = 0/0
debug_filestore = 0/0
debug_journal = 0/0
debug_ms = 0/0
debug_monc = 0/0
debug_tp = 0/0
debug_auth = 0/0
debug_finisher = 0/0
debug_heartbeatmap = 0/0
debug_perfcounter = 0/0
debug_asok = 0/0
debug_throttle = 0/0
debug_mon = 0/0
debug_paxos = 0/0
debug_rgw = 0/0
</pre>
<pre>
ceph-authtool --create-keyring /etc/ceph/ceph.mon.keyring --gen-key -n mon. --cap mon 'allow *'
ceph-authtool --create-keyring /etc/ceph/ceph.client.admin.keyring --gen-key -n client.admin --set-uid=0 --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow'
ceph-authtool /etc/ceph/ceph.mon.keyring --import-keyring /etc/ceph/ceph.client.admin.keyring
monmaptool --create --add  storage1 192.168.122.1 --add storage2 192.168.122.2 --add storage3 192.168.122.3 --fsid 97e61689-e729-45b7-baa8-7e1a36b6b863 /etc/ceph/monmap
scp -r /etc/ceph/* storage2:/etc/ceph
scp -r /etc/ceph/* storage3:/etc/ceph
</pre>
ceph monitor安装(三个节点)
<pre>
export HOSTNAME=storage{1,2,3}
ceph-mon --mkfs -i $HOSTNAME --monmap /etc/ceph/monmap --keyring /etc/ceph/ceph.mon.keyring
touch /var/lib/ceph/mon/ceph-$HOSTNAME/done
systemctl enable ceph-mon@$HOSTNAME
systemctl start ceph-mon@$HOSTNAM
</pre>
ceph osd安装(三个节点)
storage1
<pre>
ceph-disk prepare /dev/sdb
ceph-disk activate /dev/sdb1
</pre>
storage2
<pre>
ceph-disk prepare /dev/sdb
ceph-disk activate /dev/sdb1
</pre>
storage3
<pre>
ceph-disk prepare /dev/sdb
ceph-disk activate /dev/sdb1
</pre>
ceph配置openstack的资源池(storage1)
<pre>
ceph osd pool create images 3 3
ceph auth get-or-create client.glance mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=images'
ceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring
scp /etc/ceph/ceph.client.glance.keyring node2:/etc/ceph
scp /etc/ceph/ceph.client.glance.keyring node3:/etc/ceph
ceph osd pool create volumes 3 3
ceph auth get-or-create client.cinder mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=vms, allow rwx pool=images'
ceph auth get-or-create client.cinder | tee /etc/ceph/ceph.client.cinder.keyring
chown cinder.cinder /etc/ceph/ceph.client.cinder.keyring
scp /etc/ceph/ceph.client.cinder.keyring node2:/etc/ceph
scp /etc/ceph/ceph.client.cinder.keyring node3:/etc/ceph
ssh node2 chown cinder.cinder /etc/ceph/ceph.client.cinder.keyring
ssh node3 chown cinder.cinder /etc/ceph/ceph.client.cinder.keyring
</pre>

## Neutron
### 所有节点
<pre>
yum install openstack-neutron python-neutron openstack-neutron-ml2 openstack-neutron-openvswitch ebtables net-tools openstack-utils  -y
systemctl enable openvswitch
systemctl start openvswitch
systemctl enable memcached
systemctl start memcached
</pre>
### 控制节点
<pre>
mysql
CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'node' IDENTIFIED BY 'neutron';
openstack user create --domain default --project service --project-domain default --password neutron neutron
openstack service create --name neutron --description "OpenStack Networking" network
openstack endpoint create --region wuhan network public http://node:9696
openstack endpoint create --region wuhan network internal http://node:9696
openstack endpoint create --region wuhan network admin http://node:9696
openstack role add --project service --user neutron --project-domain default --user-domain default admin
</pre>
`/etc/neutron/neutron.conf`
<pre>
openstack-config --set /etc/neutron/neutron.conf DEFAULT core_plugin ml2
openstack-config --set /etc/neutron/neutron.conf DEFAULT service_plugins router
openstack-config --set /etc/neutron/neutron.conf DEFAULT transport_url rabbit://guest:guest@node
openstack-config --set /etc/neutron/neutron.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/neutron/neutron.conf DEFAULT router_distributed true
openstack-config --set /etc/neutron/neutron.conf DEFAULT l3_ha true
openstack-config --set /etc/neutron/neutron.conf DEFAULT dhcp_agents_per_network 2
openstack-config --set /etc/neutron/neutron.conf DEFAULT max_l3_agents_per_router 3
openstack-config --set /etc/neutron/neutron.conf DEFAULT min_l3_agents_per_router 3 # 根据网络节点个数配置
openstack-config --set /etc/neutron/neutron.conf database connection mysql+pymysql://neutron:neutron@node/neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_uri http://node:5000
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_url http://node:35357
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken memcached_servers 192.168.8.146:11211,192.168.8.194:11211,192.168.8.183:11211 
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken auth_type password
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken user_domain_name default      
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken project_name service
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken username neutron
openstack-config --set /etc/neutron/neutron.conf keystone_authtoken password neutron
openstack-config --set /etc/neutron/neutron.conf nova auth_url http://node:35357
openstack-config --set /etc/neutron/neutron.conf nova auth_type password
openstack-config --set /etc/neutron/neutron.conf nova project_domain_name default
openstack-config --set /etc/neutron/neutron.conf nova user_domain_name default      
openstack-config --set /etc/neutron/neutron.conf nova region_name wuhan
openstack-config --set /etc/neutron/neutron.conf nova project_name service
openstack-config --set /etc/neutron/neutron.conf nova username nova
openstack-config --set /etc/neutron/neutron.conf nova password nova
openstack-config --set /etc/neutron/neutron.conf oslo_concurrency lock_path /var/lib/neutron/tmp
</pre>
> 需要修改每个控制节点的 `bind_host`为自己的主机名

`/etc/neutron/plugins/ml2/ml2_conf.ini`
<pre>
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 type_drivers flat,vlan
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 tenant_network_types vlan,flat
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 mechanism_drivers openvswitch
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2 extension_drivers port_security 
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_flat flat_networks '*'
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini ml2_type_vlan network_vlan_ranges default:3001:4000
openstack-config --set /etc/neutron/plugins/ml2/ml2_conf.ini securitygroup enable_ipset True 
</pre>

## 网络节点
`/etc/sysctl.conf`
<pre>
net.ipv4.ip_forward=1
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
</pre>
<pre>
sysctl -p
scp -r node1:/etc/neutron/ /etc/
</pre>
<pre>
ovs-vsctl add-br br-ex
ovs-vsctl add-port br-ex ens6
</pre>
`/etc/neutron/plugins/ml2/openvswitch_agent.ini`
<pre>
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent enable_distributed_routing true 
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings default:br-ex 
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
</pre>
`/etc/neutron/l3_agent.ini`
<pre>
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver openvswitch 
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT ha_vrrp_auth_password password   ### 这一条先不执行
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT agent_mode dvr_snat
</pre>
`/etc/neutron/metadata_agent.ini`
<pre>
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip node
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret neutron
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_workers 2
</pre>
`/etc/neutron/dhcp_agent.ini`
<pre>
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT interface_driver openvswitch 
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dhcp_driver neutron.agent.linux.dhcp.Dnsmasq
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT enable_isolated_metadata True 
openstack-config --set /etc/neutron/dhcp_agent.ini DEFAULT dnsmasq_config_file /etc/neutron/dnsmasq.conf
</pre>
<pre>
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
for id in neutron-{server,openvswitch-agent,dhcp-agent,metadata-agent,l3-agent};do systemctl enable $id;systemctl start $id;done

</pre>
### 计算节点
<pre>
net.ipv4.conf.all.rp_filter=0
net.ipv4.conf.default.rp_filter=0
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
</pre>
<pre>
sysctl -p
scp -r node1:/etc/neutron /etc
ovs-vsctl add-br br-ex
ovs-vsctl add-port br-ex ens6
</pre>
`/etc/neutron/plugins/ml2/openvswitch_agent.ini`
<pre>
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini agent enable_distributed_routing true 
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini ovs bridge_mappings default:br-ex 
openstack-config --set /etc/neutron/plugins/ml2/openvswitch_agent.ini securitygroup firewall_driver neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver
</pre>
`/etc/neutron/metadata_agent.ini`
<pre>
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT nova_metadata_ip node
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_proxy_shared_secret neutron
openstack-config --set /etc/neutron/metadata_agent.ini DEFAULT metadata_workers 2
</pre>
`/etc/neutron/l3_agent.ini`
<pre>
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT agent_mode dvr
openstack-config --set /etc/neutron/l3_agent.ini DEFAULT interface_driver openvswitch 
</pre>
<pre>
for id in neutron-{openvswitch,metadata,l3}-agent;do systemctl enable $id;systemctl start $id;done
</pre>
### 所有节点
<pre>
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
</pre>
### 高可用
`/etc/haproxy/haproxy.cfg`
<pre>
########### Neutron_api_cluster ###########
listen neutron_api_cluster
bind 192.168.8.145:9696
balance  source
option  tcpka
option  httpchk
option  tcplog
server node1 192.168.8.146:9696 check inter 2000 rise 2 fall 5
server node2 192.168.8.194:9696 check inter 2000 rise 2 fall 5
server node3 192.168.8.183:9696 check inter 2000 rise 2 fall 5
</pre>


# nova
## 全部节点
<pre>
yum install openstack-nova python-nova openstack-utils -y
</pre>
`/etc/nova/nova.conf`
<pre>
openstack-config --set /etc/nova/nova.conf DEFAULT use_neutron True
openstack-config --set /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.firewall.NoopFirewallDriver
openstack-config --set /etc/nova/nova.conf DEFAULT linuxnet_interface_driver nova.network.linux_net.LinuxOVSInterfaceDriver
 
openstack-config --set /etc/nova/nova.conf DEFAULT scheduler_default_filters RetryFilter,AvailabilityZoneFilter,ComputeFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter,AggregateMultiTenancyIsolation,AggregateInstanceExtraSpecsFilter,AggregateCoreFilter,AggregateRamFilter
openstack-config --set /etc/nova/nova.conf DEFAULT reclaim_instance_interval 7200
openstack-config --set /etc/nova/nova.conf DEFAULT resize_confirm_window 1
openstack-config --set /etc/nova/nova.conf DEFAULT flat_injected True
openstack-config --set /etc/nova/nova.conf DEFAULT injected_network_template /usr/lib/python2.7/site-packages/nova/virt/interfaces.template
openstack-config --set /etc/nova/nova.conf DEFAULT force_config_drive true
 
openstack-config --set /etc/nova/nova.conf DEFAULT enabled_apis osapi_compute,metadata
openstack-config --set /etc/nova/nova.conf DEFAULT transport_url rabbit://guest:guest@node
openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_workers 2
openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen=node{1,2,3}
openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen=node{1,2,3}
openstack-config --set /etc/nova/nova.conf DEFAULT osapi_compute_listen=node{1,2,3}
openstack-config --set /etc/nova/nova.conf DEFAULT metadata_workers 2
  
openstack-config --set /etc/nova/nova.conf database connection mysql+pymysql://nova:nova@node/nova
openstack-config --set /etc/nova/nova.conf api_database connection mysql+pymysql://nova:nova@node/nova_api
openstack-config --set /etc/nova/nova.conf api auth_strategy keystone
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_uri http://node:5000
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_url http://node:35357
openstack-config --set /etc/nova/nova.conf keystone_authtoken auth_type password
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken user_domain_name default
openstack-config --set /etc/nova/nova.conf keystone_authtoken project_name service
openstack-config --set /etc/nova/nova.conf keystone_authtoken username nova
openstack-config --set /etc/nova/nova.conf keystone_authtoken password nova
  
openstack-config --set /etc/nova/nova.conf vnc enabled true
openstack-config --set /etc/nova/nova.conf vnc vncserver_listen 0.0.0.0
openstack-config --set /etc/nova/nova.conf vnc vncserver_proxyclient_address '$my_ip'
  
openstack-config --set /etc/nova/nova.conf glance api_servers http://node{1,2,3}:9292
openstack-config --set /etc/nova/nova.conf oslo_concurrency lock_path /var/lib/nova/tmp
  
openstack-config --set /etc/nova/nova.conf neutron url http://node:9696
openstack-config --set /etc/nova/nova.conf neutron auth_url http://node:35357
openstack-config --set /etc/nova/nova.conf neutron auth_type password
openstack-config --set /etc/nova/nova.conf neutron project_domain_name default
openstack-config --set /etc/nova/nova.conf neutron user_domain_name default
openstack-config --set /etc/nova/nova.conf neutron region_name wuhan
openstack-config --set /etc/nova/nova.conf neutron project_name neutron
openstack-config --set /etc/nova/nova.conf neutron username neutron
openstack-config --set /etc/nova/nova.conf neutron project_name service
openstack-config --set /etc/nova/nova.conf neutron password neutron
openstack-config --set /etc/nova/nova.conf neutron service_metadata_proxy true
openstack-config --set /etc/nova/nova.conf neutron metadata_proxy_shared_secret neutron
  
openstack-config --set /etc/nova/nova.conf cinder os_region_name wuhan
openstack-config --set /etc/nova/nova.conf cinder catalog_info volumev2:cinderv2:internalURL
openstack-config --set /etc/nova/nova.conf libvirt images_rbd_pool volumes
openstack-config --set /etc/nova/nova.conf libvirt images_rbd_ceph_conf /etc/ceph/ceph.conf
openstack-config --set /etc/nova/nova.conf libvirt rbd_user cinder
openstack-config --set /etc/nova/nova.conf libvirt rbd_secret_uuid 3947ff89-361e-4622-98fd-48b0b2723a6c
 
openstack-config --set /etc/nova/nova.conf conductor workers 2
 
openstack-config --set /etc/nova/nova.conf cache enabled true
openstack-config --set /etc/nova/nova.conf cache memcache_servers node1:11211,node2:11211,node3:11211
openstack-config --set /etc/nova/nova.conf cache backend oslo_cache.memcache_pool
openstack-config --set /etc/nova/nova.conf cache debug_cache_backend true
openstack-config --set /etc/nova/nova.conf cache expiration_time 600

### 如果是虚拟机环境还需要下面两个
openstack-config --set /etc/nova/nova.conf libvirt virt_type qemu
openstack-config --set /etc/nova/nova.conf libvirt cpu_mode none
</pre>

> 另外，需要给每台机器配置 `my_ip`

## 控制节点
<pre>
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://'$my_ip':6080/vnc_auto.html
mysql
CREATE DATABASE nova_api;
CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'node' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'node' IDENTIFIED BY 'nova';
openstack service create --name nova --description "OpenStack Compute" compute
openstack service create --name placement --description "Placement API" placement
openstack endpoint create --region wuhan compute public http://node:8774/v2.1
openstack endpoint create --region wuhan compute internal http://node:8774/v2.1
openstack endpoint create --region wuhan compute admin http://node:8774/v2.1
openstack user create --domain default --project service --project-domain default --password nova nova
openstack role add --project service --user nova --project-domain default --user-domain default admin
su -s /bin/sh -c "nova-manage db sync" nova
su -s /bin/sh -c "nova-manage api_db sync" nova
for id in openstack-nova-{conductor,api,scheduler,consoleauth,novncproxy};do systemctl enable $id;systemctl start $id;done
</pre>
### 高可用
`/etc/haproxy/haproxy.cfg`
<pre>

</pre>
## 计算节点
<pre>
openstack-config --set /etc/nova/nova.conf vnc novncproxy_base_url http://'$my_ip':6080/vnc_auto.html
systemctl enable libvirtd
systemctl start libvirtd
</pre>

























## Cinder-Volume
<pre>
yum install pacemaker pcs corosync fence-agents resource-agents libqb0 crmsh -y
systemctl enable pcsd
systemctl start pcsd
echo hacluster |passwd --stdin hacluster
pcs cluster auth node1 node2 node3 -u hacluster -p hacluster --force
pcs cluster setup --force --name openstack-cluster node1 node2 node3
pcs cluster start --all
pcs cluster enable --all
pcs property set pe-warn-series-max=1000 \
  pe-input-series-max=1000 \
  pe-error-series-max=1000 \
  cluster-recheck-interval=5min
pcs property set stonith-enabled=false
pcs resource create volume_vip ocf:heartbeat:IPaddr2 \
  params ip="192.168.8.149" cidr_netmask="24" op monitor interval="30s"
pcs resource create openstack-cinder-api systemd:openstack-cinder-api --clone interleave=true
pcs resource create openstack-cinder-scheduler systemd:openstack-cinder-scheduler --clone interleave=true
pcs resource create openstack-cinder-volume systemd:openstack-cinder-volume

pcs constraint order start openstack-cinder-api-clone then openstack-cinder-scheduler-clone
pcs constraint colocation add openstack-cinder-scheduler-clone with openstack-cinder-api-clone
pcs constraint order start openstack-cinder-scheduler-clone then openstack-cinder-volume
pcs constraint colocation add openstack-cinder-volume with openstack-cinder-scheduler-clone
cd /usr/lib/ocf/resource.d/openstack
wget https://git.openstack.org/cgit/openstack/openstack-resource-agents/plain/ocf/cinder-api
chmod a+rx *
crm configure
primitive p_cinder-api ocf:openstack:cinder-api \
   params config="/etc/cinder/cinder.conf" \
   os_password="admin" \
   os_username="admin" \
   os_tenant_name="admin" \
   keystone_get_token_url="http://node:5000/v2.0/tokens" \
   op monitor interval="30s" timeout="30s"
commit
</pre>
<pre>
openstack-config --set /etc/cinder/cinder.conf DEFAULT host cinder-cluster
openstack-config --set /etc/cinder/cinder.conf DEFAULT osapi_volume_listen 192.168.8.149
openstack-config --set /etc/cinder/cinder.conf DEFAULT auth_strategy keystone
openstack-config --set /etc/cinder/cinder.conf DEFAULT control_exchange cinder
openstack-config --set /etc/cinder/cinder.conf database connection mysql://cinder:cinder@volume/cinder
</pre>
<pre>
crm configure
primitive p_cinder-api ocf:openstack:cinder-api \
   params config="/etc/cinder/cinder.conf" \
   os_password="admin" \
   os_username="admin" \
   os_tenant_name="admin" \
   keystone_get_token_url="http://node:35357/v3" \
   op monitor interval="30s" timeout="30s"
commit
</pre>


### [pacemaker知识点](http://clusterlabs.org/doc/zh-CN/Pacemaker/1.1/html/Clusters_from_Scratch/_adding_a_resource.html)
<pre>
# pcs resource create ClusterIP ocf:heartbeat:IPaddr2 \
    ip=192.168.0.120 cidr_netmask=32 op monitor interval=30s
</pre>
这里是 ocf:heartbeat:IPaddr2 的其他重要信息。

另外一个重要的信息是 ocf:heartbeat:IPaddr2。这告诉Pacemaker三件事情，第一个部分ocf，指明了这个资源采用的标准(类型)以及在哪能找到它。第二个部分标明这个资源脚本的在ocf中的名字空间，在这个例子中是heartbeat。最后一个部分指明了资源脚本的名称。
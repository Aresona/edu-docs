# OpenStack企业私有云实践

理解OpenStack架构和各个服务的功能、组件、运行原理才是最重要的。

## OpenStack介绍
### OpenStack版本历史

OpenStack是一个开源的云计算管理平台项目，由几个主要的组件组合起来完成具体工作。支持几乎所有类型的云环境，项目目标是提供实施简单、可大规模扩展、丰富、标准统一的云计算管理平台。OpenStack通过各种互补的服务提供了基础设施即服务（IaaS）的解决方案，每个服务提供API以进行集成。

OpenStack最早包含两个模块：Nova和Swift,这两个是可选的两个组件。发展迅速主要是因为有非常多的场商加入。

它的版本是用地名来命名的，街道的名字来命名的，现在是L版。真正稳定是G版，I版是最后一个支持centos6和python2.6的版本，后面就是python2.7的版本了。

[官网](docs.openstack.org)


OpenStack项目主要管理了三大资源：计算、网络、存储资源。

计算：OpenStack可以管理大量的云主机，可以按需使用资源，可以弹性地进行扩张

存储资源：可以管理硬盘。

网络：真正使用很少用SDN,


### OpenStack组件及功能：

Horizon:基于OpenStack API接口使用django开发的web管理

Nova: 通过虚拟化技术提供计算资源池

Neutron: 实现了虚拟机的网络资源管理

#### Storage(存储)

Object Storage(Swift): 对象存储，适用于“一次写入、多次读取”

Block Storage(Cinder): 块存储，提供存储资源池

#### Shared Services（共享服务）

Identify Service(Keystone): 认证管理

Image Service(Glance)： 提供虚拟镜像的注册和存储管理

Telemetry(Ceilometer): 提供监控和数据采集、计量服务 

#### Higher-level servceis(高层服务)

Orchestration(Heat): 自动化部署的组件

Database Service(Trove)： 提供数据库应用服务

主要使用python来开发


## OpenStack基础环境


两台虚拟机：
linux-node1.example.com   192.168.56.11   控制节点
linux-node2.example.com   192.168.56.12   计算节点

#### 时间同步

	yum install chrony -y
	sed -i '22aallow 192.168/16' /etc/chrony.conf
	systemctl enable chronyd.service
	systemctl start chronyd.service
	timedatectl set-timezone Asia/Shanghai
	
#### Base
<pre>
yum install -y http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
yum install centos-release-openstack-liberty -y
yum install python-openstackclient -y
</pre>
**制作keystone源**
<pre>
echo "[keystone]
name=keystone
baseurl=http://mirrors.aliyun.com/centos/7.2.1511/cloud/x86_64/openstack-liberty/
enabled=1
gpgcheck=0" > /etc/yum.repos.d/keystone.repo
</pre>

#### MySQL

	yum install mariadb mariadb-server MySQL-python -y
	/bin/cp /usr/share/mysql/my-medium.cnf /etc/my.cnf
	vim /etc/my.cnf	
	[mysqld]
	default-storage-engine = innodb
	innodb_file_per_table
	collation-server = utf8_general_ci
	init-connect = 'SET NAMES utf8'
	character-set-server = utf8
	systemctl enable mariadb.service
	systemctl start mariadb.service
	mysql_secure_installation
	## 设置密码为123456

##### SQL语句

<pre>
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'keystone';

CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'glance';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'glance';

CREATE DATABASE nova;
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'nova';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'nova';

CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'neutron';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'neutron';

CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';
show databases;
quit
</pre>

#### 消息队列RabbitMQ

SOA  两个角色，服务的提供者，服务的消费者 松耦合

<pre>
yum install rabbitmq-server -y
systemctl enable rabbitmq-server.service
systemctl start rabbitmq-server.service
netstat -lntup|grep 5672
</pre>

给rabbitmq新建一个用户并授权 

<pre>
rabbitmqctl add_user openstack openstack
rabbitmqctl set_permissions openstack ".*" ".*" ".*"
### 这里是所有vhost的所有权限
</pre>

rabbitmq默认有很多插件，其中有一个是WEB的插件，可以通过WEB界面来查看rabbitmq的状态

#### 启动rabbitmq的web插件
<pre>
rabbitmq-plugins list
rabbitmq-plugins enable rabbitmq_management
systemctl restart rabbitmq-server.service
## 默认用户名密码：guest/guest,这时前面授权的openstack还不能用；默认是有TAGS是administrator的用户才可以登录，所以需要在Admin里面更新openstack用户的tags。
</pre>

#### rabbitmq的重要性

它是整个openstack架构里面扮演的是交通枢纽的角色，并且它是支持集群的。

#### rabbitmq监控

通过页面的最下角的HTTP API就可以进行监控；可以监控哪个队列有没有被堵。

### Keystone

#### 安装
<pre>
yum install -y openstack-keystone httpd mod_wsgi memcached python-memcached
</pre>
> 在用户名密码通过keystone认证后会产生一个tocken,以前这个tocken都是记录在一个表里面的，时间长了的话这个表就会很大；在新的版本中，用memcache来实现这个存储功能。

#### 配置

上传配置文件到/opt下
<pre>
unzip config.zip
</pre>
修改相关配置文件
<pre>
/etc/keystone/keystone.conf
[DEFAULT]
admin_token  ## 使用token连接到keystone
[database]
connection = mysql://keystone:keystone@192.168.56.11/keystone
[memcache]
servers = 192.168.56.11:11211
[token]
provider = uuid
driver = memcache
[revoke]
driver = sql
</pre>
所有配置
<pre>
[root@linux-node1 yum.repos.d]# grep '^[a-Z]' /etc/keystone/keystone.conf 
admin_token = 863d35676a5632e846d9
connection = mysql://keystone:keystone@192.168.56.11/keystone
servers = 192.168.56.11:11211
driver = sql
provider = uuid
driver = memcache
</pre>
> 还有一行是关于 `debug` 的配置 `verbose = true`
做一个随机码来当做token
<pre>
openssl rand -hex 10
</pre>

#### 同步及启动

##### 创建keystone数据库表
<pre>
su -s /bin/sh -c "keystone-manage db_sync" keystone
# 执行完后就会在/var/log/keystone下面生成日志，可以发现日志是属于keystone用户的
# db_sync             Sync the database.
</pre>

##### 启动keystone

以前启动是通过自己的一个python的服务，但是性能不好；后来使用APACHE来启动
###### 启动memcached
<pre>
systemctl enable memcached.service
systemctl start memecached.service
</pre>
###### 启动Apache

<pre>
cat /etc/httpd/conf.d/wsgi-keystone.conf
Listen 5000
Listen 35357

<VirtualHost *:5000>
    WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-public
    WSGIScriptAlias / /usr/bin/keystone-wsgi-public
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/httpd/keystone-error.log
    CustomLog /var/log/httpd/keystone-access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>

<VirtualHost *:35357>
    WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone group=keystone display-name=%{GROUP}
    WSGIProcessGroup keystone-admin
    WSGIScriptAlias / /usr/bin/keystone-wsgi-admin
    WSGIApplicationGroup %{GLOBAL}
    WSGIPassAuthorization On
    <IfVersion >= 2.4>
      ErrorLogFormat "%{cu}t %M"
    </IfVersion>
    ErrorLog /var/log/httpd/keystone-error.log
    CustomLog /var/log/httpd/keystone-access.log combined

    <Directory /usr/bin>
        <IfVersion >= 2.4>
            Require all granted
        </IfVersion>
        <IfVersion < 2.4>
            Order allow,deny
            Allow from all
        </IfVersion>
    </Directory>
</VirtualHost>
</pre>
> 这里开了两个虚拟主机，一个是5000端口，是给正常的API来访问的，35357是给管理的API使用的。

修改/etc/httpd/conf/httpd.conf配置 `servername`

<pre>
ServerName 192.168.56.11:80
</pre>
启动
<pre>
systemctl enable httpd.service
systemctl start httpd.service
</pre>

#### 权限管理

设置两个环境变量 

<pre>
export OS_TOKEN=863d35676a5632e846d9
export OS_URL=http://192.168.56.11:35357/v3
export OS_IDENTITY_API_VERSION=3
</pre>

这时就可以连上keystone了，在新版本中，连接keystone的时候命令都改成了openstack

##### openstack命令使用
* 查看keystone有哪些用户 
<pre>
openstack user list
</pre>
* 创建一个租户
<pre>
openstack project create --domain default   --description "Admin Project" admin
</pre>
* 创建admin用户
<pre>
openstack user create --domain default --password-prompt admin
</pre>
> 这里设置密码为admin,但是在生产中一定要复杂，它后面是可以登录到dashboard上面的。


* 创建admin角色

<pre>
openstack role create admin
</pre>
* 关联admin用户、admin项目（租户）和admin角色
<pre>
openstack role add --project admin --user admin admin
</pre>

* 创建demo的项目，作为普通用户，后面通过这个用户来执行一些操作

<pre>
openstack project create --domain default --description "Demo Project" demo
openstack user create --domain default --password=demo demo
openstack role create user
openstack role add --project demo --user demo user
openstack project create --domain default --description "Service Project" service
</pre>
> 这里的Service Project是为了把以后的nova等服务加进去。

* 查看创建结果

<pre>
[root@linux-node1 ~]# openstack user list
+----------------------------------+-------+
| ID                               | Name  |
+----------------------------------+-------+
| 3e59d7ff78ae4b53a28c4c66456c0e73 | demo  |
| beaa5b94925845868e67bb7de63e0ea2 | admin |
+----------------------------------+-------+
[root@linux-node1 ~]# openstack role list
+----------------------------------+-------+
| ID                               | Name  |
+----------------------------------+-------+
| 3d1031b90b3e45718e12fcb4675db62d | user  |
| 8f79ee46270d4e2ebbac38d3c0751f94 | admin |
+----------------------------------+-------+
[root@linux-node1 ~]# openstack --help|grep list^C
[root@linux-node1 ~]# openstack project list
+----------------------------------+---------+
| ID                               | Name    |
+----------------------------------+---------+
| 9bc39520b1e44036988f286d95975639 | admin   |
| e77a70dde9e442469a82021b19cf0d75 | service |
| ec5d6ae9a57841ef97d22715cc52ed6d | demo    |
+----------------------------------+---------+
</pre>
> 只有admin和user两个角色 ，它不是随便配置的

#### 服务目录及注册服务 

* 注册keystone
<pre>
openstack service create --name keystone --description "OpenStack Identity" identity
</pre>
* 注册keystone的endpoint
<pre>
openstack endpoint create --region RegionOne identity public http://192.168.56.11:5000/v2.0
openstack endpoint create --region RegionOne identity internal http://192.168.56.11:5000/v2.0
openstack endpoint create --region RegionOne identity admin http://192.168.56.11:35357/v2.0
</pre>
* 删除endpoint节点

<pre>
openstack endpoint delete id
</pre>
endpoint有三种类型

1. 公共的（放在互联网上，对公众可见）
2. 内部的
3. 管理的

> 区分这三种API是为了更加灵活地调用各种API。

#### 测试

因为前面使用了admin_token的方式，但现在我们已经有了用户名密码了，所以先把前面的环境变量去掉，然后用我们的用户名和密码来进行验证。

<pre>
unset OS_TOKEN
unset OS_URL
</pre>
##### 使用参数方式获取token

<pre>
openstack --os-auth-url http://192.168.56.11:35357/v3 \
--os-project-domain-id default --os-user-domain-id default \
--os-project-name admin --os-username admin --os-auth-type password \
token issue
</pre>

> 这里返回结果才说明keystone成功

##### 配置 `keystone` 环境变量来代替上面这一段长长的命令方便执行*

###### 配置admin环境变量

<pre>
[root@linux-node1 ~]# vim admin-openrc.sh
export OS_PROJECT_DOMAIN_ID=default
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_NAME=admin
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=admin
export OS_AUTH_URL=http://192.168.56.11:35357/v3
export OS_IDENTITY_API_VERSION=3
</pre>

###### 配置demo环境变量 
<pre>
[root@linux-node1 ~]# vim demo-openrc.sh
export OS_PROJECT_DOMAIN_ID=default
export OS_USER_DOMAIN_ID=default
export OS_PROJECT_NAME=demo
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=demo
export OS_AUTH_URL=http://192.168.56.11:5000/v3
export OS_IDENTITY_API_VERSION=3
</pre>

###### 加执行权限

	chmod +x admin-openrc.sh demo-openrc.sh

以后使用的时候只需要在当前shell里面source一下就可以了

<pre>
[root@linux-node1 ~]# source admin-openrc.sh 
[root@linux-node1 ~]# openstack token issue
+------------+----------------------------------+
| Field      | Value                            |
+------------+----------------------------------+
| expires    | 2016-07-08T08:55:14.760284Z      |
| id         | 2f69696055bc4364abe08541c1f17622 |
| project_id | 9bc39520b1e44036988f286d95975639 |
| user_id    | beaa5b94925845868e67bb7de63e0ea2 |
+------------+----------------------------------+
</pre>

### glance

* Glance主要由三个服务构成：glance-api、glance-registry以及image store。
* Glance-api:接受云系统镜像的创建、删除、读取请求。
* Glance-Registry:云系统的镜像注册服务

#### 安装

	yum install -y openstack-glance python-glance python-glanceclient

#### 配置  

配置涉及到API和Registry两个部分

glance一定会连接数据库，也一定会连接keystone,还有一个是存储的位置，还有日志相关的配置

##### 数据库相关

<pre>
/etc/glance/glance-api.conf
[database]
connection=mysql://glance:glance@192.168.56.11/glance
</pre>

<pre>
/etc/glance/glance-registry.conf
[database]
connection=mysql://glance:glance@192.168.56.11/glance
</pre>

###### 同步数据库

<pre>
su -s /bin/sh -c "glance-manage db_sync" glance
</pre>

> 这里有个提示如下，可以忽略

	No handlers could be found for logger "oslo_config.cfg"

##### 去keystone认证

<pre>
source admin-openrc.sh
openstack user create --domain default --password=glance glance
openstack role add --project service --user glance admin

openstack service create --name glance --description "OpenStack Image service" image
openstack endpoint create --region RegionOne image public http://192.168.56.11:9292
openstack endpoint create --region RegionOne image internal http://192.168.56.11:9292
openstack endpoint create --region RegionOne image admin http://192.168.56.11:9292
</pre>

##### 配置keystone相关
<pre>
vim /etc/glance/glance-api.conf
[DEFAULT]
notification_driver = noop
verbose=True
[keystone_authtoken]
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = glance
password = glance
[paste_deploy]
flavor=keystone
[glance_store]
default_store=file
filesystem_store_datadir=/var/lib/glance/images/
</pre>

<pre>
[keystone_authtoken]
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = glance
password = glance
[paste_deploy]
flavor=keystone
</pre>
> glance是不需要用到消息队列的，

#### 所有配置

<pre>
[root@linux-node1 ~]# grep '^[a-Z]' /etc/glance/glance-api.conf 
verbose=True
notification_driver = noop
connection=mysql://glance:glance@192.168.56.11/glance
default_store=file
filesystem_store_datadir=/var/lib/glance/images/
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = glance
password = glance
flavor=keystone
</pre>

<pre>
[root@linux-node1 ~]# grep '^[a-Z]' /etc/glance/glance-registry.conf 
connection=mysql://glance:glance@192.168.56.11/glance
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = glance
password = glance
flavor=keystone
</pre>

#### 启动服务

<pre>
systemctl enable openstack-glance-api
systemctl enable openstack-glance-registry
systemctl start openstack-glance-api
systemctl start openstack-glance-registry
</pre>

#### 更新环境变量 

<pre>
echo "export OS_IMAGE_API_VERSION=2" \
| tee -a admin-openrc.sh demo-openrc.sh
</pre>


#### 上传一个镜像

<pre>
cd /server/tools
wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
glance image-create --name "cirros" \
--file cirros-0.3.4-x86_64-disk.img \
--disk-format qcow2 --container-format bare \
--visibility public --progress
</pre>

#### 验证

<pre>
glance image-list
</pre>

### Nova控制节点

#### Nova必备的组件

* API：负责接收和响应外部请求。支持OpenStackAPI,EC2API。
* Cert: 负责身份认证。
* Scheduler: 用于云主机调度。
* Conductor: 计算节点访问数据的中间件。
* Consoleauth: 用于控制台的授权验证。
* Novncproxy: VNC代理

#### NovaAPI

* nova-api组件实现了RESTful API功能，是外部访问Nova的唯一途径。
* 接收外部的请求并通过Message Queue将请求发送给其他的服务组件，同时也兼容EC2API，所以也可以用EC2的管理工具对nova进行日常管理。

#### Nova Scheduler

Nova Scheduler模块在openstack中的作用就是决策虚拟机创建在哪个主机（计算节点）上。

决策一个虚机应该调度到某物理节点，需要分两个步骤：

* 过滤（Filter）
* 计算权值（Weight）

fileter Scheduler首先得到未经过滤的主机列表，然后根据过滤属性，选择服务条件的计算节点主机。

经过主机过滤后，需要对主机进行权值的计算，根据策略选择相应的某一台主机（对于每一个要创建的虚拟机而言）

#### nova安装

<pre>
yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler python-novaclient -y
</pre>

#### nova配置

##### 需要配置的东西 

1. 数据库
2. keystone
3. rabbitMQ
4. 网络相关
5. 注册

###### 数据库
<pre>
[database]
connection=mysql://nova:nova@192.168.56.11/nova
su -s /bin/sh -c "nova-manage db sync" nova
</pre>
###### rabbixMQ
<pre>
[DEFAULT]
rpc_backend=rabbit
[oslo_messaging_rabbit]
rabbit_host=192.168.56.11
rabbit_port=5672
rabbit_userid=openstack
rabbit_password=openstack
</pre>

###### keystone
<pre>
source admin-openrc.sh
openstack user create --domain default --password=nova nova
openstack role add --project service --user nova admin
</pre>
<pre>
[DEFAULT]
auth_strategy=keystone

[keystone_authtoken]
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = nova
password = nova
</pre>

> `/usr/lib/python2.7/site-packages/nova/network/neutronv2/api.py` 其实是一个类

###### 其他

<pre>
[DEFAULT]
network_api_class=nova.network.neutronv2.api.API
security_group_api=neutron
linuxnet_interface_driver=nova.network.linux_net.NeutronLinuxBridge
firewall_driver=nova.virt.libvirt.firewall.NoopFirewallDriverInterfaceDriver
enabled_apis=osapi_compute,metadata
</pre>
###### VNC

<pre>
[DEFAULT]
my_ip=192.168.56.11
[vnc]
vncserver_listen=$my_ip 
vncserver_proxyclient_address=$my_ip
</pre>

###### glance
<pre>
[glance]
host=$my_ip
</pre>

##### 控制节点全部配置

<pre>
[root@linux-node1 tools]# grep '^[a-Z]' /etc/nova/nova.conf 
my_ip=192.168.56.11
enabled_apis=osapi_compute,metadata
auth_strategy=keystone
network_api_class=nova.network.neutronv2.api.API
linuxnet_interface_driver=nova.network.linux_net.NeutronLinuxBridgeInterfaceDriver
security_group_api=neutron
firewall_driver=nova.virt.libvirt.firewall.NoopFirewallDriver
rpc_backend=rabbit
connection=mysql://nova:nova@192.168.56.11/nova
host=$my_ip
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = nova
password = nova
rabbit_host=192.168.56.11
rabbit_port=5672
rabbit_userid=openstack
rabbit_password=openstack
vncserver_listen=$my_ip
vncserver_proxyclient_address=$my_ip
</pre>
#### 启动
<pre>
systemctl enable openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
systemctl start openstack-nova-api.service openstack-nova-cert.service openstack-nova-consoleauth.service openstack-nova-scheduler.service openstack-nova-conductor.service openstack-nova-novncproxy.service
</pre>
#### 注册
<pre>
openstack service create --name nova --description "OpenStack Compute" compute
openstack endpoint create --region RegionOne compute public http://192.168.56.11:8774/v2/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute internal http://192.168.56.11:8774/v2/%\(tenant_id\)s
openstack endpoint create --region RegionOne compute admin http://192.168.56.11:8774/v2/%\(tenant_id\)s
openstack host list
</pre>

### nova计算节点

* nova-compute 一般运行在计算节点上，通过Message Queue接收并管理VM的生命周期
* nova-compute通过Libvirt管理KVM，通过XenAPI管理Xen等。

#### 安装
<pre>
yum install openstack-nova-compute -y
systemctl start libvirtd openstack-nova-compute
systemctl enable libvirtd openstack-nova-compute
</pre>
#### 计算节点配置
<pre>
[DEFAULT]
my_ip=192.168.56.12
[vnc]
novncproxy_base_url=http://192.168.56.11:6080/vnc_auto.html
vncserver_listen=0.0.0.0
enabled=true
keymap=en-us
[glance]
host=192.168.56.11
[libvirt]
virt_type=kvm
</pre>
##### 基础环境配置
<pre>
grep -E 'vmx|svm' /proc/cpuinfo
yum install -y chrony
echo 'server 192.168.56.11 iburst' > /etc/chrony.conf
timedatectl set-timezone Asia/Shanghai
systemctl start chronyd.service
systemctl enable chronyd.service
</pre>


##### 验证（控制节点上查看）
<pre>
[root@linux-node1 ~]# openstack host list
+-------------------------+-------------+----------+
| Host Name               | Service     | Zone     |
+-------------------------+-------------+----------+
| linux-node1.example.com | cert        | internal |
| linux-node1.example.com | conductor   | internal |
| linux-node1.example.com | consoleauth | internal |
| linux-node1.example.com | scheduler   | internal |
| linux-node2.example.com | compute     | nova     |
+-------------------------+-------------+----------+
[root@linux-node1 ~]# nova image-list
+--------------------------------------+--------+--------+--------+
| ID                                   | Name   | Status | Server |
+--------------------------------------+--------+--------+--------+
| e4b42550-7615-4c64-b7cf-78ddc2a17545 | cirros | ACTIVE |        |
+--------------------------------------+--------+--------+--------+
[root@linux-node1 ~]# nova endpoints
WARNING: nova has no endpoint in ! Available endpoints for this service:
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | 37d7b5468c484ad4aee98b0e62bb5308                              |
| interface | internal                                                      |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://192.168.56.11:8774/v2/9bc39520b1e44036988f286d95975639 |
+-----------+---------------------------------------------------------------+
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | a2b1c827286d4634ad2646b108e20aea                              |
| interface | public                                                        |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://192.168.56.11:8774/v2/9bc39520b1e44036988f286d95975639 |
+-----------+---------------------------------------------------------------+
+-----------+---------------------------------------------------------------+
| nova      | Value                                                         |
+-----------+---------------------------------------------------------------+
| id        | cd5c66cefb1d46c6831e83697b44a122                              |
| interface | admin                                                         |
| region    | RegionOne                                                     |
| region_id | RegionOne                                                     |
| url       | http://192.168.56.11:8774/v2/9bc39520b1e44036988f286d95975639 |
+-----------+---------------------------------------------------------------+
WARNING: keystone has no endpoint in ! Available endpoints for this service:
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | 0400393337324e66b247249fde191d60 |
| interface | admin                            |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:35357/v2.0  |
+-----------+----------------------------------+
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | 52025170ee314428839767ad73c32c5d |
| interface | internal                         |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:5000/v2.0   |
+-----------+----------------------------------+
+-----------+----------------------------------+
| keystone  | Value                            |
+-----------+----------------------------------+
| id        | ea1d0891d8ff4c5ea41224cdaadaf382 |
| interface | public                           |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:5000/v2.0   |
+-----------+----------------------------------+
WARNING: glance has no endpoint in ! Available endpoints for this service:
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | 1c7196ad815546c7a748077f22e70153 |
| interface | internal                         |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:9292        |
+-----------+----------------------------------+
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | cadcbc129f284392b7581dedef5ab10d |
| interface | admin                            |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:9292        |
+-----------+----------------------------------+
+-----------+----------------------------------+
| glance    | Value                            |
+-----------+----------------------------------+
| id        | ea64e9c68f99442b9be74c594b63b59e |
| interface | public                           |
| region    | RegionOne                        |
| region_id | RegionOne                        |
| url       | http://192.168.56.11:9292        |
+-----------+----------------------------------+
</pre> 


### 控制节点

	echo "linux-node2.example.com" > /etc/hostname
	cat >> /etc/hosts <<EOF
	192.168.56.11	linux-node1.example.com
	192.168.56.12	linux-node2.example.com
	EOF
#### 时间同步

### 计算节点

	echo "linux-node1.example.com" > /etc/hostname
	cat >> /etc/hosts <<EOF
	192.168.56.11	linux-node1.example.com
	192.168.56.12	linux-node2.example.com
	EOF
	ntpdate ntp1.aliyun.com
	scp /etc/nova/nova.conf 192.168.56.12:/etc/nova/


> 注意的点： selinux、iptables、/etc/hosts、主机名、

## 服务学习

### keystone

keystone是一个验证服务，它主要有两个作用，一个是用户认证，另外一个是服务目录

* 用户与认证：用户权限与用户行为跟踪
* 服务目录：提供一个服务目录，包括所有服务项与相关API的端点

> 在安装的时候所有的服务都需要在keystone上做服务的注册。这样别的服务才能调用。

#### 相关名词说明

##### 用户认证相关
User: 用户

Tenant: 租户（项目），它是一个可以访问资源的组合。

Token: 令牌

Role: 角色（代表一个权限的组合）

##### 服务目录相关
Service: 服务

Endpoint: 端点（连接方式）

### Neutron

Nova-Network---> Quantum ---> Neutron

网络也需要两个节点
#### 控制节点

* 安装

<pre>
yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge python-neutronclient ebtables ipset -y
</pre>
* 注册

<pre>
openstack service create --name neutron --description "OpenStack Networking" network
openstack endpoint create --region RegionOne network public http://192.168.56.11:9696
openstack endpoint create --region RegionOne network internal http://192.168.56.11:9696
openstack endpoint create --region RegionOne network admin http://192.168.56.11:9696
</pre>

* 配置

<pre>
[DEFAULT]
core_plugin = ml2
service_plugins = router
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://192.168.56.11:8774/v2
[database]
connection = mysql://neutron:neutron@192.168.56.11:3306/neutron
[keystone_authtoken]
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = neutron
password = neutron
[oslo_messaging_rabbit]
rabbit_host = 192.168.56.11
rabbit_port = 5672
rabbit_userid = openstack
rabbit_password = openstack
[nova]
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
region_name = RegionOne
project_name = service
username = nova 
password = nova
[oslo_concurrency]
lock_path = $state_path/lock
</pre>
<pre>
/etc/neutron/plugins/ml2/ml2_config.ini
[root@linux-node1 ml2]# grep '^[a-z]' ml2_conf.ini 
type_drivers = flat,vlan,gre,vxlan,geneve
tenant_network_types = vlan,gre,vxlan,geneve
mechanism_drivers = openvswitch,linuxbridge
extension_drivers = port_security
flat_networks = physnet1
enable_ipset = True
</pre>
<pre>
/etc/neutron/plugins/ml2/linuxbridge_agent.ini
[root@linux-node1 ml2]# grep '^[a-z]' linuxbridge_agent.ini 
physical_interface_mappings = physnet1:eth0
enable_vxlan = false
prevent_arp_spoofing = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
enable_security_group = True
</pre>
<pre>
[root@linux-node1 ml2]# grep '^[a-z]' /etc/neutron/dhcp_agent.ini 
interface_driver = neutron.agent.linux.interface.BridgeInterfaceDriver
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = True
</pre>
<pre>
[root@linux-node1 neutron]# grep '^[a-z]' metadata_agent.ini 
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_region = RegionOne
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = neutron
password = neutron
nova_metadata_ip = 192.168.56.11
metadata_proxy_shared_secret = neutron
</pre>
<pre>
/etc/nova/nova.conf
[neutron]
url = http://192.168.56.11:9696
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
service_metadata_proxy=true
metadata_proxy_shared_secret = neutron
</pre>
<pre>
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
</pre>
* 创建keystone用户

<pre>
source admin-openrc.sh
openstack user create --domain default --password=neutron neutron
openstack role add --project service --user neutron admin
</pre>

<pre>
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron
</pre>
<pre>
systemctl restart openstack-nova-api
systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
systemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service
</pre>
* 验证

<pre>
[root@linux-node1 ~]# neutron agent-list
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host                    | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
| 618654ed-67b7-4f32-b87b-63b8614f3c70 | Metadata agent     | linux-node1.example.com | :-)   | True           | neutron-metadata-agent    |
| b6fdaa48-5457-4211-a134-35321236144c | Linux bridge agent | linux-node1.example.com | :-)   | True           | neutron-linuxbridge-agent |
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
</pre>


#### 计算节点
<pre>
yum install openstack-neutron openstack-neutron-linuxbridge ebtables ipset -y
</pre>

##### 配置

<pre>
scp /etc/neutron/neutron.conf 192.168.56.12:/etc/neutron
scp /etc/neutron/plugins/ml2/linuxbridge_agent.ini 192.168.56.12:/etc/neutron/plugins/ml2/
scp /etc/neutron/plugins/ml2/ml2_conf.ini 192.168.56.12:/etc/neutron/plugins/ml2/
</pre>
<pre>
/etc/nova/nova.conf
[neutron]
url = http://192.168.56.11:9696
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
region_name = RegionOne
project_name = service
username = neutron
password = neutron
</pre>
<pre>
[root@linux-node2 neutron]# grep '^[a-z]' /etc/neutron/plugins/ml2/linuxbridge_agent.ini
physical_interface_mappings = physnet1:eth0
enable_vxlan = false
prevent_arp_spoofing = True
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
enable_security_group = True
</pre>
<pre>
[root@linux-node2 neutron]# grep '^[a-z]' /etc/neutron/neutron.conf
state_path = /var/lib/neutron
core_plugin = ml2
service_plugins = router
auth_strategy = keystone
notify_nova_on_port_status_changes = True
notify_nova_on_port_data_changes = True
nova_url = http://192.168.56.11:8774/v2
rpc_backend=rabbit
auth_uri = http://192.168.56.11:5000
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
project_name = service
username = neutron
password = neutron
connection = mysql://neutron:neutron@192.168.56.11:3306/neutron
auth_url = http://192.168.56.11:35357
auth_plugin = password
project_domain_id = default
user_domain_id = default
region_name = RegionOne
project_name = service
username = nova
password = nova
lock_path = $state_path/lock
rabbit_host = 192.168.56.11
rabbit_port = 5672
rabbit_userid = openstack
rabbit_password = openstack
</pre>
<pre>
[root@linux-node2 neutron]# grep '^[a-z]' /etc/neutron/plugins/ml2/ml2_conf.ini 
type_drivers = flat,vlan,gre,vxlan,geneve
tenant_network_types = vlan,gre,vxlan,geneve
mechanism_drivers = openvswitch,linuxbridge
extension_drivers = port_security
flat_networks = physnet1
enable_ipset = True
</pre>

<pre>
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
systemctl enable neutron-linuxbridge-agent.service
systemctl start neutron-linuxbridge-agent.service
</pre>
<pre>
[root@linux-node1 ~]# neutron agent-list
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
| id                                   | agent_type         | host                    | alive | admin_state_up | binary                    |
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
| 3683be02-efe3-4ab4-83a9-1c2c121de67c | DHCP agent         | linux-node1.example.com | :-)   | True           | neutron-dhcp-agent        |
| 618654ed-67b7-4f32-b87b-63b8614f3c70 | Metadata agent     | linux-node1.example.com | :-)   | True           | neutron-metadata-agent    |
| b6fdaa48-5457-4211-a134-35321236144c | Linux bridge agent | linux-node1.example.com | :-)   | True           | neutron-linuxbridge-agent |
| d72668fa-c2c2-4ffc-8b3b-702062b5f2af | Linux bridge agent | linux-node2.example.com | :-)   | True           | neutron-linuxbridge-agent |
+--------------------------------------+--------------------+-------------------------+-------+----------------+---------------------------+
</pre>
##### 创建网络

<pre>
node1下执行
source admin-openrc.sh
neutron net-create flat --shared --provider:physical_network physnet1 --provider:network_type flat
</pre>
###### 创建一个子网
<pre>
neutron subnet-create flat 192.168.56.0/24 --name flat-subnet --allocation-pool start=192.168.56.100,end=192.168.56.200 --dns-nameserver 192.168.56.2 --gateway 192.168.56.2
[root@linux-node1 ~]# neutron subnet-list
+--------------------------------------+-------------+-----------------+------------------------------------------------------+
| id                                   | name        | cidr            | allocation_pools                                     |
+--------------------------------------+-------------+-----------------+------------------------------------------------------+
| d998dd66-4b6c-4ace-8af1-d56f910bc412 | flat-subnet | 192.168.56.0/24 | {"start": "192.168.56.100", "end": "192.168.56.200"} |
+--------------------------------------+-------------+-----------------+------------------------------------------------------+
</pre>

### 关闭虚拟机的DHCP服务 

> 编辑 --> 虚拟网络编辑器 --> NAT模式 --> 使用本地DHCP服务将IP地址分配给虚拟机（勾去掉）

### 创建一个虚拟机

需要的东西：内存、CPU、磁盘、登录方式（key）等。
<pre>
source demo-openrc.sh
ssh-keygen -q -N ""
nova keypair-add --pub-key .ssh/id_rsa.pub mykey
nova keypair-list
nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0
nova secgroup-add-rule default tcp 22 22 0.0.0.0/0
</pre>

#### 列举相关资源

<pre>
nova flavor-list
nova image-list
neutron net-list
nova secgroup-list
</pre>
<pre>
[root@linux-node1 ~]# neutron net-list
+--------------------------------------+------+------------------------------------------------------+
| id                                   | name | subnets                                              |
+--------------------------------------+------+------------------------------------------------------+
| 18d16cc9-e32d-470e-b179-8946b6502d8c | flat | d998dd66-4b6c-4ace-8af1-d56f910bc412 192.168.56.0/24 |
+--------------------------------------+------+------------------------------------------------------+
这里网络ID后面会用到（必须用ID）
</pre>

#### 开始创建
<pre>
nova boot --flavor m1.tiny --image cirros --nic net-id=18d16cc9-e32d-470e-b179-8946b6502d8c --security-group default --key-name mykey hello-instance
</pre>

#### 查看是否创建成功
<pre>
nova list
ssh cirros@192.168.56.101
</pre>

#### web界面查看
<pre>
nova get-vnc-console hello-instance novnc
</pre>
# 端口统计

服务 |  端口
----|---|
rabbitmq|   5672
rabbitmq-web|15672
keystone-admin|35357
keystone-general|5000
glance-api|9292
glance-registry|9191
novpnproxy|6080





















# JIRA和CONFLUENCE使用

scrum 把所有的事分小了去做

最短的时间内交付最多的价值

整个团队里面三个角色，一般七个成员左右，

Product Owner       Scrum Team        Scrum Master

########## Artifacts

produc Backlog    需求列表

epic  形容大的需求点，

theam

user story

从大到小的需求点

Sprint Backlog   当前迭代要做的东西 ，迭代完了后就是Increments

Daily scrum

Sprint Review   看准上线版本

Sprint Restrospective   团队内部交流

## JIRA使用

* 项目计划
* 项目执行
* 项目跟踪



### 项目计划（）

创建项目－－项目设置              录入需求、创建任务     创建子任务  （这两个就是issue）
Jira Admin   Project Manager   Project Manager(product Manager/Engineer/Designer)

### 项目执行

### 项目跟踪

把大事拆成小事，每个小事做好才能做好大事

